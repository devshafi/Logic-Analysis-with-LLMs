{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b85adadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipelines\n",
    "from accelerate import init_empty_weights\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bf347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Checking if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b19ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a predicate logic assistant specializing in formal logic analysis.\n",
    "\n",
    "Your capabilities include:\n",
    "1. Evaluating the validity of logical arguments using predicate logic\n",
    "2. Identifying logical fallacies in arguments\n",
    "3. Converting natural language statements to predicate logic notation\n",
    "4. Determining if formulas are well-formed\n",
    "5. Checking logical equivalence between formulas\n",
    "6. Performing logical operations (negation, conjunction, disjunction, implication)\n",
    "7. Translating between natural language and first-order logic\n",
    "\n",
    "When given a logical problem and query, respond ONLY with \"yes\" or \"no\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46f76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_path = \"/home/fozle/Course Work/Y1S2/CISC 844/Projects/LLM-Predicate-Logic/dataset/data.csv\"\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "86774270",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 50  # Adjust based on how many examples you want to test\n",
    "df_sample = df.sample(sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "01037876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d48fbd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a predicate logic assistant specializing in formal logic analysis.\n",
    "\n",
    "Your capabilities include:\n",
    "1. Evaluating the validity of logical arguments using predicate logic\n",
    "2. Identifying logical fallacies in arguments\n",
    "3. Converting natural language statements to predicate logic notation\n",
    "4. Determining if formulas are well-formed\n",
    "5. Checking logical equivalence between formulas\n",
    "6. Performing logical operations (negation, conjunction, disjunction, implication)\n",
    "7. Translating between natural language and first-order logic\n",
    "\n",
    "When given a logical query, respond ONLY with \"yes\" if the statement is logically correct else \"no\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b505adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",  # or 3B version\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "505aed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following premises: It is cloudy or Richard is playing tennis. If it is cloudy, then it is sunny. If Richard plays tennis, then it is sunny. Can we infer the following from them?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  load only one row from the dataset\n",
    "row = df_sample.iloc[0]\n",
    "# user_prompt = row['query']\n",
    "user_prompt = \"Consider the following premises: It is cloudy or Richard is playing tennis. If it is cloudy, then it is sunny. If Richard plays tennis, then it is sunny. Can we infer the following from them?\"\n",
    "print(user_prompt)\n",
    "ground_truth = row['answer']\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "946cd416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fozle/Course Work/Y1S2/CISC 844/Projects/LLM-Predicate-Logic/venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/fozle/Course Work/Y1S2/CISC 844/Projects/LLM-Predicate-Logic/venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Prepare messages for the model\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=128,  # Shorter since we only need yes/no\n",
    "            do_sample=False  # Deterministic response\n",
    "        )\n",
    "        \n",
    "        \n",
    "# Extract response content\n",
    "response = outputs[0][\"generated_text\"][-1][\"content\"].strip().lower()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "17c43573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df8b04abca3450886595032174f1d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing model:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Run inference on samples with fixed progress tracking\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Testing model\"):\n",
    "    # Construct user prompt from problem and query\n",
    "    user_prompt = f\"{row['problem']} {row['query']}\"\n",
    "    \n",
    "    # Prepare messages for the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Get model response\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        outputs = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=128,  # Shorter since we only need yes/no\n",
    "            do_sample=False  # Deterministic response\n",
    "        )\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Extract response content\n",
    "        response = outputs[0][\"generated_text\"][-1][\"content\"].strip().lower()\n",
    "        \n",
    "        # Check if response contains yes or no\n",
    "        if \"yes\" in response and \"no\" not in response:\n",
    "            prediction = \"yes\"\n",
    "        elif \"no\" in response:\n",
    "            prediction = \"no\"\n",
    "        else:\n",
    "            prediction = \"unclear\"\n",
    "            \n",
    "        # Compare with ground truth\n",
    "        correct = prediction == row[\"answer\"]\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"logic\": row[\"logic\"],\n",
    "            \"rule_category\": row[\"rule_category\"],\n",
    "            \"rule\": row[\"rule\"],\n",
    "            \"problem\": row[\"problem\"],\n",
    "            \"query\": row[\"query\"],\n",
    "            \"ground_truth\": row[\"answer\"],\n",
    "            \"prediction\": prediction,\n",
    "            \"correct\": correct,\n",
    "            \"inference_time\": inference_time\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "149852c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>logic</th>\n",
       "      <th>rule_category</th>\n",
       "      <th>rule</th>\n",
       "      <th>problem</th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>correct</th>\n",
       "      <th>inference_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1127</td>\n",
       "      <td>propositional</td>\n",
       "      <td>inference</td>\n",
       "      <td>disjunction elimination</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Consider the following premises: It is cloudy ...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "      <td>0.029729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>predicate</td>\n",
       "      <td>equivalent</td>\n",
       "      <td>Law of quantifier distribution</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Consider the following premises: There is at l...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0.015643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3261</td>\n",
       "      <td>propositional</td>\n",
       "      <td>inference</td>\n",
       "      <td>resolution</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Consider the following premises: Jennifer is r...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0.014647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1737</td>\n",
       "      <td>predicate</td>\n",
       "      <td>equivalent</td>\n",
       "      <td>existential biconditional laws</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Consider the following premises: There is at l...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0.014481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4527</td>\n",
       "      <td>predicate</td>\n",
       "      <td>equivalent</td>\n",
       "      <td>universal distributive laws</td>\n",
       "      <td>inference</td>\n",
       "      <td>Consider the following premises: For all x, x ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "      <td>0.014226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id          logic rule_category                            rule  \\\n",
       "0  1127  propositional     inference         disjunction elimination   \n",
       "1    80      predicate    equivalent  Law of quantifier distribution   \n",
       "2  3261  propositional     inference                      resolution   \n",
       "3  1737      predicate    equivalent  existential biconditional laws   \n",
       "4  4527      predicate    equivalent     universal distributive laws   \n",
       "\n",
       "         problem                                              query  \\\n",
       "0      unrelated  Consider the following premises: It is cloudy ...   \n",
       "1  contradiction  Consider the following premises: There is at l...   \n",
       "2  contradiction  Consider the following premises: Jennifer is r...   \n",
       "3  contradiction  Consider the following premises: There is at l...   \n",
       "4      inference  Consider the following premises: For all x, x ...   \n",
       "\n",
       "  ground_truth prediction  correct  inference_time  \n",
       "0           no         no     True        0.029729  \n",
       "1           no        yes    False        0.015643  \n",
       "2           no        yes    False        0.014647  \n",
       "3           no        yes    False        0.014481  \n",
       "4          yes         no    False        0.014226  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9f2951f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1becd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 44.00%\n",
      "\n",
      "Prediction counts:\n",
      "  yes: 36\n",
      "  no: 14\n",
      "\n",
      "Accuracy by Logic Type:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>accuracy_pct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predicate</th>\n",
       "      <td>37</td>\n",
       "      <td>37.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>propositional</th>\n",
       "      <td>13</td>\n",
       "      <td>61.538462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count  accuracy_pct\n",
       "logic                             \n",
       "predicate         37     37.837838\n",
       "propositional     13     61.538462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy by Rule Category:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>accuracy_pct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rule_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inference</th>\n",
       "      <td>25</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equivalent</th>\n",
       "      <td>17</td>\n",
       "      <td>41.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fallacy</th>\n",
       "      <td>8</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count  accuracy_pct\n",
       "rule_category                     \n",
       "inference         25     56.000000\n",
       "equivalent        17     41.176471\n",
       "fallacy            8     12.500000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = results_df[\"correct\"].mean() * 100\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Display number of samples by prediction\n",
    "prediction_counts = results_df[\"prediction\"].value_counts()\n",
    "print(\"\\nPrediction counts:\")\n",
    "for pred, count in prediction_counts.items():\n",
    "    print(f\"  {pred}: {count}\")\n",
    "\n",
    "# # Display accuracy by logic type\n",
    "print(\"\\nAccuracy by Logic Type:\")\n",
    "logic_acc = results_df.groupby(\"logic\")[\"correct\"].agg([\"mean\", \"count\"])\n",
    "logic_acc[\"accuracy_pct\"] = logic_acc[\"mean\"] * 100\n",
    "display(logic_acc[[\"count\", \"accuracy_pct\"]])\n",
    "\n",
    "# # Display accuracy by rule category\n",
    "print(\"\\nAccuracy by Rule Category:\")\n",
    "rule_acc = results_df.groupby(\"rule_category\")[\"correct\"].agg([\"mean\", \"count\"])\n",
    "rule_acc[\"accuracy_pct\"] = rule_acc[\"mean\"] * 100\n",
    "display(rule_acc.sort_values(\"count\", ascending=False)[[\"count\", \"accuracy_pct\"]])\n",
    "\n",
    "# # Save results to CSV\n",
    "results_file = \"llama_predicate_logic_results.csv\"\n",
    "results_df.to_csv(results_file, index=False)\n",
    "print(f\"\\nResults saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d25de495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct\n",
       "False    28\n",
       "True     22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.value_counts(\"correct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
